---
title: "Week 4 Codes - Cross-Validation"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)

library(tidyverse)
```

#### ---------------------------------------------------------------------------
### K-Fold CV: Ames Housing Dataset

```{r,message=FALSE}
ames <- readRDS("AmesHousing.rds")   # load dataset
```


```{r, message = FALSE}
# split data

set.seed(042023)  # fix the random number generator for reproducibility

library(caret)  # load library

train_index <- createDataPartition(y = ames$Sale_Price, p = 0.8, list = FALSE) # split available data into 80% training and 20% test datasets

ames_train <- ames[train_index,]   # training data

ames_test <- ames[-train_index,]   # test data
```


```{r}
# define CV specifications

cv_specs <- trainControl(method = "repeatedcv",   # CV method
                         number = 10,    # number of folds
                         repeats = 1)     # number of repeats
```


```{r}
# $k$-fold CV with the first model

m1 <- train(form = Sale_Price ~ Garage_Area,    # specify model
            data = ames_train,   # specify dataset
            method = "lm",       # specify type of model
            trControl = cv_specs,   # CV specifications
            metric = "RMSE")   # metric to evaluate model
# "RMSE" the error that the model will make
```

```{r}
m1   # summary of CV
```

```{r}
m1$results  # estimate and variability of metrics
#the RMSE might vary by RMSESD (13,603)
```

```{r}
m1$resample   # results from all folds, all repeats
```


```{r}
# $k$-fold CV with the second model

m2 <- train(form = Sale_Price ~ Overall_Qual,  
            data = ames_train,          
            method = "lm",              
            trControl = cv_specs,       
            metric = "RMSE")           

m2

m2$results
```


```{r}
# $k$-fold CV with the third model

m3 <- train(form = Sale_Price ~ Garage_Area + Year_Built + Overall_Qual,  
            data = ames_train,
            method = "lm",
            trControl = cv_specs,
            metric = "RMSE")

m3

m3$results
```


```{r, fig.align='center', fig.height=6, fig.width=8}
# compare CV results for different models

# create data frame to plot results
df <- data.frame(model_number = 1:3, RMSE = c(m1$results$RMSE,  
                                             m2$results$RMSE,
                                             m3$results$RMSE))

# plot results from CV
ggplot(data = df, aes(x = model_number, y =  RMSE)) +   
  geom_point() + geom_line()

```


```{r}
# after choosing final (optimal) model, refit final model using ALL training data

m3$finalModel    # final model
```


```{r}
# obtain estimate of prediction error from test data

final_model_preds <- predict(object = m3, newdata = ames_test)   # obtain predictions on test data

pred_error_est <- sqrt(mean((ames_test$Sale_Price - final_model_preds)^2))    # calculate RMSE from test data

pred_error_est
#error of around $35,038
```


```{r, fig.align='center', fig.height=6, fig.width=8}
# variable importance

library(vip)

vip(object = m3,         # CV object 
    num_features = 20,   # maximum number of predictors to show importance for
    method = "model")            # model-specific VI scores
```


#### ---------------------------------------------------------------------------
### K-Fold CV: Auto Dataset (PRACTICE)

```{r,message=FALSE}
library(ISLR2)  # load library

data("Auto")   # load dataset
```


```{r, message = FALSE}
# split data

set.seed(042023)  # fix the random number generator for reproducibility

library(caret)  # load library

train_index <- createDataPartition(y = Auto$mpg, p = 0.8, list = FALSE) # split available data into 80% training and 20% test datasets

Auto_train <- Auto[train_index,]   # training data

Auto_test <- Auto[-train_index,]   # test data
```


```{r}
# define CV specifications

cv_specs <- trainControl(method = "repeatedcv",   # CV method
                         number = 5,    # number of folds
                         repeats = 5)     # number or repeats
```


```{r}
# specify grid of 'k' values to search over

k_grid <- expand.grid(k = seq(1, 100, by = 1))
```


```{r}
# train the KNN model using CV to find optimal 'k'

knn_cv <- train(form = mpg ~ horsepower, 
                 data = Auto_train, 
                 method = "knn",
                 trControl = cv_specs, 
                 tuneGrid = k_grid,
                 metric = "RMSE")
```


```{r}
knn_cv   # CV results

knn_cv$bestTune #optimal value of K

min(knn_cv$results$RMSE) # corresponding optimal RMSE

knn_cv$resample  # this shows results of all folds all repeats for the optimal K

mean(knn_cv$resample$RMSE) # another way to get the optimal RMSE (averaging over all folds all repeats)
```


```{r, fig.align='center'}
ggplot(knn_cv)   # plot CV results of RMSE for different 'k'

# left of the plot is underfit while right is overfit becuase when K increases bias increases and variance descreases.
```


```{r}
# build final model with optimal 'k' chosen from CV

knn_cv$finalModel   # final model

# obtain predictions on test data
final_model_preds <- predict(object = knn_cv, newdata = Auto_test)
# final_model <- knnreg(formula = mpg ~ horsepower, data = Auto_train, k = knn_cv$bestTune$K) another way

# estimate test set prediction error
sqrt(mean((Auto_test$mpg - final_model_preds)^2))    # RMSE
```

#### ---------------------------------------------------------------------------